### BERT

#### O que é?

BERT ("Bidirectional Encoder Representations from Transformers") é um modelo de NLP desenvolvido pela Google em 2018 para melhorar o sistema de buscas deles. 

*** Características ***

- É bidirecional, porque pega o significado de uma palavra avaliando as palavras anteriores e posteriores.
- Possui um mecanismo de atenção ("self-attencion") que permite um melhor entendimento de relações entre palavras pelo contexto.
- É pré-treinado.
- Ele usa aprendizado semi-supervisionado, o que significa que ele foi treinado para entender os padrões de linguagem e pode ser utilizado em conjunto com outros modelos em tarefas de aprendizado supervisionado.

#### Para que pode ser utilizado?

- Escolha da palavra que mais se encaixa em uma frase, avaliando o contexto.
- Geração de respostas para questões (estado da arte 2022).
- Tarefas de classificação: análise de sentimentos (estado da arte 2022), detecção de ironia.
- Reconhecimento de entidades nomeadas ("Named Entity Recognition").
- Inferência de linguagem (estado da arte 2022).
- Similaridade semântica (estado da arte 2022).
- "Paraphrase detection" (estado da arte 2022).
- "Linguistic Acceptability" (estado da arte 2022).

#### Como funciona?



#### Vantagens

- O BERT precisa de menos dados de treinamento do que um modelo criado do zero, por ser pré-treinado.

#### Desvantagens

- 

## Referências

- https://arxiv.org/pdf/1706.03762.pdf
- https://www.kaggle.com/code/pavansanagapati/knowledge-graph-nlp-tutorial-bert-spacy-nltk/notebook
